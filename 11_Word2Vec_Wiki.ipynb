{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "11_Word2Vec_Wiki.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_DeBirxSoyr",
        "colab_type": "text"
      },
      "source": [
        "# **Word2Vec練習，以維基百科為例**\n",
        "自然語言處理NLP(Natural Language Processing) 是人工智慧最主要的應用之一，近年來在深度學習領域取得重大進展，其中Word2Vec模型扮演極重要的角色。\n",
        "\n",
        "Word2Vec是 2013 年由 Tomas Mikolov 等人所提出，透過大量資料的訓練，將深度學習訓練的權重向量，用來表達字詞的語意，讓語意相似的單字可以有較近的距離。\n",
        "本範例以2020/3/1所下載的中文維基百科為範例，用來建立Word2Vec語意模型並且預測相關離的字詞\n",
        "\n",
        "（由於Wiki的語料庫含有簡體中文，將會轉換為繁體後再進行模型訓練）"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RieNDQ55Slor",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 759
        },
        "outputId": "c4cc940c-0ee9-4c34-9c9b-c11182b1194b"
      },
      "source": [
        "# 安裝簡轉繁工具 - opencc-python-reimplemented\n",
        "!pip install opencc-python-reimplemented\n",
        "# 使用wget指令，從wiki獲取語料庫\n",
        "!wget https://dumps.wikimedia.org/zhwiki/20200301/zhwiki-20200301-pages-articles-multistream1.xml-p1p162886.bz2\n",
        "\n",
        "# import gensim 對維基百科處理的類 WikiCorpus\n",
        "from gensim.corpora import WikiCorpus\n",
        "\n",
        "wiki_corpus = WikiCorpus('zhwiki-20200301-pages-articles-multistream1.xml-p1p162886.bz2', dictionary={})\n",
        "next(iter(wiki_corpus.get_texts()))[:10]\n",
        "\n",
        "# 初始化text_num（詞的數量）\n",
        "text_num = 0\n",
        "\n",
        "# 將維基裡的每篇文章轉換位1行text文字，並且去掉標點符號等內容\n",
        "with open('wiki_text.txt', 'w', encoding='utf-8') as f:\n",
        "    for text in wiki_corpus.get_texts():\n",
        "        f.write(' '.join(text)+'\\n')\n",
        "        text_num += 1\n",
        "        if text_num % 10000 == 0:\n",
        "            print('{} articles processed.'.format(text_num))\n",
        "\n",
        "    print('{} articles processed.'.format(text_num))\n",
        "\n",
        "# import jieba 用以之後分詞\n",
        "import jieba\n",
        "from opencc import OpenCC\n",
        "\n",
        "\n",
        "# Initial\n",
        "# 初始化\n",
        "# 將wiki的語料庫簡轉繁且分詞後，再將其儲存，檔名為seg.txt\n",
        "cc = OpenCC('s2t')\n",
        "train_data = open('wiki_text.txt', 'r', encoding='utf-8').read()\n",
        "train_data = cc.convert(train_data)\n",
        "train_data = jieba.lcut(train_data)\n",
        "train_data = [word for word in train_data if word != '']\n",
        "train_data = ' '.join(train_data)\n",
        "open('seg.txt', 'w', encoding='utf-8').write(train_data)\n",
        "\n",
        "from gensim.models import word2vec\n",
        "\n",
        "\n",
        "# Settings\n",
        "# 設定各項參數\n",
        "seed = 666           # 亂數種子，用於random時，與詞向量有關\n",
        "sg = 0               # 設定訓練算法，為0時採用CBOW算法、為1時採用skip-gram算法\n",
        "window_size = 10     # 周圍詞彙要看多少範圍\n",
        "vector_size = 100    # 轉成向量的維度\n",
        "min_count = 1        # 詞頻少於 min_count 之詞彙不會參與訓練\n",
        "workers = 8          # 訓練的並行數量\n",
        "epochs = 5           # 訓練的迭代次數\n",
        "batch_words = 10000  # 每次給予多少詞彙量訓練\n",
        "\n",
        "# 讀入上方已經分詞好的語料庫，並且也設定好各項參數\n",
        "train_data = word2vec.LineSentence('seg.txt')\n",
        "model = word2vec.Word2Vec(\n",
        "    train_data,\n",
        "    min_count=min_count,\n",
        "    size=vector_size,\n",
        "    workers=workers,\n",
        "    iter=epochs,\n",
        "    window=window_size,\n",
        "    sg=sg,\n",
        "    seed=seed,\n",
        "    batch_words=batch_words\n",
        ")\n",
        "\n",
        "model.save('word2vec.model')\n",
        "\n",
        "\n",
        "from gensim.models import word2vec\n",
        "\n",
        "string = '微生物'\n",
        "model = word2vec.Word2Vec.load('word2vec.model')\n",
        "print(string)\n",
        "\n",
        "for item in model.wv.most_similar(string):\n",
        "    print(item)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting opencc-python-reimplemented\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/53/0c/c499c86a719c925a08586085a56f92f3235c03ee8b4db2e59c1e9aab3f55/opencc-python-reimplemented-0.1.5.tar.gz (482kB)\n",
            "\r\u001b[K     |▊                               | 10kB 26.8MB/s eta 0:00:01\r\u001b[K     |█▍                              | 20kB 31.6MB/s eta 0:00:01\r\u001b[K     |██                              | 30kB 34.5MB/s eta 0:00:01\r\u001b[K     |██▊                             | 40kB 38.2MB/s eta 0:00:01\r\u001b[K     |███▍                            | 51kB 40.1MB/s eta 0:00:01\r\u001b[K     |████                            | 61kB 42.3MB/s eta 0:00:01\r\u001b[K     |████▊                           | 71kB 40.3MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 81kB 40.3MB/s eta 0:00:01\r\u001b[K     |██████                          | 92kB 39.8MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 102kB 40.3MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 112kB 40.3MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 122kB 40.3MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 133kB 40.3MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 143kB 40.3MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 153kB 40.3MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 163kB 40.3MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 174kB 40.3MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 184kB 40.3MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 194kB 40.3MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 204kB 40.3MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 215kB 40.3MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 225kB 40.3MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 235kB 40.3MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 245kB 40.3MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 256kB 40.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 266kB 40.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 276kB 40.3MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 286kB 40.3MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 296kB 40.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 307kB 40.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 317kB 40.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 327kB 40.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 337kB 40.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 348kB 40.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 358kB 40.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 368kB 40.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 378kB 40.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 389kB 40.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 399kB 40.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 409kB 40.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 419kB 40.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 430kB 40.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 440kB 40.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 450kB 40.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 460kB 40.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 471kB 40.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 481kB 40.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 491kB 40.3MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: opencc-python-reimplemented\n",
            "  Building wheel for opencc-python-reimplemented (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for opencc-python-reimplemented: filename=opencc_python_reimplemented-0.1.5-py2.py3-none-any.whl size=485664 sha256=8d451874e7669142031dd283ede4b8ad51e2229bcbdc157ed89d63e75d0f136d\n",
            "  Stored in directory: /root/.cache/pip/wheels/36/a0/10/888b9ac7f10154caaa6a73270b1f085e0a7b241baa672bbe49\n",
            "Successfully built opencc-python-reimplemented\n",
            "Installing collected packages: opencc-python-reimplemented\n",
            "Successfully installed opencc-python-reimplemented-0.1.5\n",
            "--2020-04-24 13:31:54--  https://dumps.wikimedia.org/zhwiki/20200301/zhwiki-20200301-pages-articles-multistream1.xml-p1p162886.bz2\n",
            "Resolving dumps.wikimedia.org (dumps.wikimedia.org)... 208.80.154.7, 2620:0:861:1:208:80:154:7\n",
            "Connecting to dumps.wikimedia.org (dumps.wikimedia.org)|208.80.154.7|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 172586252 (165M) [application/octet-stream]\n",
            "Saving to: ‘zhwiki-20200301-pages-articles-multistream1.xml-p1p162886.bz2’\n",
            "\n",
            "zhwiki-20200301-pag 100%[===================>] 164.59M  4.80MB/s    in 34s     \n",
            "\n",
            "2020-04-24 13:32:28 (4.81 MB/s) - ‘zhwiki-20200301-pages-articles-multistream1.xml-p1p162886.bz2’ saved [172586252/172586252]\n",
            "\n",
            "10000 articles processed.\n",
            "20000 articles processed.\n",
            "27590 articles processed.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Building prefix dict from the default dictionary ...\n",
            "Dumping model to file cache /tmp/jieba.cache\n",
            "Loading model cost 0.725 seconds.\n",
            "Prefix dict has been built successfully.\n",
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "微生物\n",
            "('細菌', 0.8789703845977783)\n",
            "('菌', 0.8646241426467896)\n",
            "('代謝', 0.8630419969558716)\n",
            "('藻類', 0.8562043905258179)\n",
            "('真菌', 0.8535659313201904)\n",
            "('胰島素', 0.8511801362037659)\n",
            "('免疫', 0.8462857604026794)\n",
            "('致病', 0.8450480699539185)\n",
            "('抗藥性', 0.8393200039863586)\n",
            "('激素', 0.8370355367660522)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}